{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data99666\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/aistudio/external-libraries’: File exists\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/d1/41/e6495bd7d3781cee623ce23ea6ac73282a373088fcd0ddc809a047b18eae/beautifulsoup4-4.9.3-py3-none-any.whl (115kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 14.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2; python_version >= \"3.0\" (from beautifulsoup4)\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/36/69/d82d04022f02733bf9a72bc3b96332d360c0c5307096d76f6bb7489f7e57/soupsieve-2.2.1-py3-none-any.whl\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.9.3 soupsieve-2.2.1\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/bs4 already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/beautifulsoup4-4.9.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve-2.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/aistudio/external-libraries/soupsieve already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!unzip -oq /home/aistudio/data/data99666/A_Pred.zip -d data/\r\n",
    "!unzip -oq /home/aistudio/data/data99666/dataset.zip -d data/\r\n",
    "#!unzip -oq /home/aistudio/data/data99666/paddle-seg-semi1.zip -d external-libraries\r\n",
    "\r\n",
    "#注意，第一次运行若在external-libraries目录下没有paddle_seg-semi1的文件夹，请取消第三行的注释，然后会在external-libraries下面生成一个paddle-seg-semi1的文件，\r\n",
    "#麻烦一下rename 成paddle_seg-semi1，这是paddleseg源码被我修改后的，我添加了自己的网络，损失函数等."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 解压B榜测试集\r\n",
    "!unzip -oq /home/aistudio/data/data99666/B榜测试数据集.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')\n",
    "sys.path.append('/home/aistudio/external-libraries/paddle_seg_semi1') # 我额外加的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "本次比赛，我们队伍一共使用了4个模型，通过四个模型的加权融合后得出最好的结果，下面分别介绍每个模型\n",
    "\n",
    "这些都是基于paddleseg套件进行修改的，所以按照paddleseg的方法就行模型训练和预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "与其说是四个模型，不如说是3种骨干网络，2种解码网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "四个模型的模型文件可在  \n",
    "\n",
    "external-libraries/paddle_seg_semi1/paddleseg/models/FMFNetwork.py\n",
    "\n",
    "external-libraries/paddle_seg_semi1/paddleseg/models/ACFFNetwork.py\n",
    "\n",
    "external-libraries/paddle_seg_semi1/paddleseg/models/ACFF_ViT.py\n",
    "\n",
    "external-libraries/paddle_seg_semi1/paddleseg/models/Next_ACFF.py\n",
    "\n",
    "看到\n",
    "\n",
    "四个配置文件可在external-libraries/paddle_seg_semi1/configs/semi 看到\n",
    "\n",
    "分别叫 fcn_FMF_semi_50k.yml、fcn_ACFFNet_semi_50k.yml、 fcn_ACFFViT.yml、 fcn_NextACFF.yml \n",
    "\n",
    "数据处理没啥可看的 可在external-libraries/paddle_seg_semi1/paddleseg/datasets/semi.py 看到\n",
    "\n",
    "**另外，严正声明，比赛所用的模型，除骨干网络外，皆为本人所提出的模型算法，其中ACFFNetwork.py中除骨干网络外的模型为本人所投sci（在大修）文章中的模型稍加修改后的模型， FMFNet中除骨干网络外的模型为本人正在写sci的模型稍加修改后的模型，\n",
    "与baseline 没一点相同**\n",
    "\n",
    "**另外，本人十分愿意为paddleseg提供比赛冠军模型，成为PPDE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "因为4个模型，所以预测时间略长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 下面详解每个模型的训练与预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################### FMF ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'external-libraries/paddle_seg_semi1/'\n",
      "/home/aistudio/external-libraries/paddle_seg_semi1\n",
      "/home/aistudio/external-libraries/paddle_seg_semi1/paddleseg/models/ACFF_ViT.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Callable\n",
      "2021-08-03 12:09:25 [INFO]\t\n",
      "------------Environment Information-------------\n",
      "platform: Linux-4.4.0-150-generic-x86_64-with-debian-stretch-sid\n",
      "Python: 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0]\n",
      "Paddle compiled with cuda: True\n",
      "NVCC: Cuda compilation tools, release 10.1, V10.1.243\n",
      "cudnn: 7.6\n",
      "GPUs used: 1\n",
      "CUDA_VISIBLE_DEVICES: None\n",
      "GPU: ['GPU 0: Tesla V100-SXM2-32GB']\n",
      "GCC: gcc (Ubuntu 7.5.0-3ubuntu1~16.04) 7.5.0\n",
      "PaddlePaddle: 2.1.0\n",
      "OpenCV: 4.1.1\n",
      "------------------------------------------------\n",
      "2021-08-03 12:09:27 [INFO]\t\n",
      "---------------Config Information---------------\n",
      "batch_size: 32\n",
      "iters: 36000\n",
      "learning_rate:\n",
      "  decay:\n",
      "    end_lr: 0.0001\n",
      "    power: 0.9\n",
      "    type: poly\n",
      "  value: 0.01\n",
      "loss:\n",
      "  coef:\n",
      "  - 1\n",
      "  types:\n",
      "  - ignore_index: 255\n",
      "    type: SemiLoss\n",
      "model:\n",
      "  backbone:\n",
      "    pretrained: /home/aistudio/data/data99666/Res2Net200_vd_26w_4s_ssld_pretrained.pdparams\n",
      "    type: Res2Net\n",
      "  num_classes: 2\n",
      "  type: FMFModel\n",
      "optimizer:\n",
      "  momentum: 0.9\n",
      "  type: sgd\n",
      "  weight_decay: 0.0005\n",
      "train_dataset:\n",
      "  dataset_root: /home/aistudio/data\n",
      "  mode: train\n",
      "  transforms:\n",
      "  - max_rotation: 30\n",
      "    max_scale_factor: 1.5\n",
      "    min_scale_factor: 0.5\n",
      "    size:\n",
      "    - 256\n",
      "    - 256\n",
      "    translation_offset: 128\n",
      "    type: RandomAffine\n",
      "  - type: RandomHorizontalFlip\n",
      "  - type: RandomVerticalFlip\n",
      "  - brightness_range: 0.5\n",
      "    contrast_range: 0.5\n",
      "    saturation_range: 0.5\n",
      "    type: RandomDistort\n",
      "  - type: RandomBlur\n",
      "  - mean:\n",
      "    - 0.485\n",
      "    - 0.456\n",
      "    - 0.406\n",
      "    std:\n",
      "    - 0.229\n",
      "    - 0.224\n",
      "    - 0.225\n",
      "    type: Normalize\n",
      "  type: SEMI\n",
      "val_dataset:\n",
      "  dataset_root: /home/aistudio/data\n",
      "  mode: val\n",
      "  transforms:\n",
      "  - target_size:\n",
      "    - 256\n",
      "    - 256\n",
      "    type: Resize\n",
      "  - mean:\n",
      "    - 0.485\n",
      "    - 0.456\n",
      "    - 0.406\n",
      "    std:\n",
      "    - 0.229\n",
      "    - 0.224\n",
      "    - 0.225\n",
      "    type: Normalize\n",
      "  type: SEMI\n",
      "------------------------------------------------\n",
      "W0803 12:09:27.500844 15219 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0803 12:09:27.500916 15219 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n",
      "2021-08-03 12:09:33 [INFO]\tLoading pretrained model from /home/aistudio/data/data99666/Res2Net200_vd_26w_4s_ssld_pretrained.pdparams\n",
      "2021-08-03 12:09:36 [INFO]\tThere are 1685/1685 variables loaded into Res2Net.\n",
      "init finished!\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:641: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n",
      "2021-08-03 12:11:04 [INFO]\t[TRAIN] epoch=1, iter=100/36000, loss=0.7809, lr=0.009975, batch_cost=0.8681, reader_cost=0.00792, ips=36.8634 samples/sec | ETA 08:39:23\n",
      "2021-08-03 12:12:30 [INFO]\t[TRAIN] epoch=1, iter=200/36000, loss=0.7561, lr=0.009951, batch_cost=0.8668, reader_cost=0.00018, ips=36.9158 samples/sec | ETA 08:37:12\n",
      "2021-08-03 12:13:57 [INFO]\t[TRAIN] epoch=1, iter=300/36000, loss=0.7367, lr=0.009926, batch_cost=0.8674, reader_cost=0.00019, ips=36.8911 samples/sec | ETA 08:36:06\n",
      "2021-08-03 12:15:23 [INFO]\t[TRAIN] epoch=1, iter=400/36000, loss=0.6938, lr=0.009901, batch_cost=0.8604, reader_cost=0.00017, ips=37.1902 samples/sec | ETA 08:30:31\n",
      "2021-08-03 12:16:49 [INFO]\t[TRAIN] epoch=1, iter=500/36000, loss=0.6320, lr=0.009876, batch_cost=0.8645, reader_cost=0.00018, ips=37.0172 samples/sec | ETA 08:31:28\n",
      "2021-08-03 12:18:15 [INFO]\t[TRAIN] epoch=1, iter=600/36000, loss=0.5965, lr=0.009852, batch_cost=0.8595, reader_cost=0.00018, ips=37.2290 samples/sec | ETA 08:27:07\n",
      "2021-08-03 12:19:42 [INFO]\t[TRAIN] epoch=1, iter=700/36000, loss=0.5650, lr=0.009827, batch_cost=0.8632, reader_cost=0.00017, ips=37.0705 samples/sec | ETA 08:27:51\n",
      "2021-08-03 12:21:07 [INFO]\t[TRAIN] epoch=1, iter=800/36000, loss=0.5495, lr=0.009802, batch_cost=0.8567, reader_cost=0.00017, ips=37.3546 samples/sec | ETA 08:22:34\n",
      "2021-08-03 12:22:34 [INFO]\t[TRAIN] epoch=1, iter=900/36000, loss=0.5447, lr=0.009777, batch_cost=0.8630, reader_cost=0.00018, ips=37.0798 samples/sec | ETA 08:24:51\n",
      "2021-08-03 12:24:00 [INFO]\t[TRAIN] epoch=1, iter=1000/36000, loss=0.5420, lr=0.009752, batch_cost=0.8632, reader_cost=0.00018, ips=37.0722 samples/sec | ETA 08:23:31\n",
      "2021-08-03 12:25:27 [INFO]\t[TRAIN] epoch=1, iter=1100/36000, loss=0.5305, lr=0.009728, batch_cost=0.8677, reader_cost=0.00019, ips=36.8802 samples/sec | ETA 08:24:41\n",
      "2021-08-03 12:26:53 [INFO]\t[TRAIN] epoch=1, iter=1200/36000, loss=0.5210, lr=0.009703, batch_cost=0.8627, reader_cost=0.00017, ips=37.0911 samples/sec | ETA 08:20:23\n",
      "2021-08-03 12:28:19 [INFO]\t[TRAIN] epoch=1, iter=1300/36000, loss=0.5281, lr=0.009678, batch_cost=0.8591, reader_cost=0.00018, ips=37.2481 samples/sec | ETA 08:16:50\n",
      "2021-08-03 12:29:45 [INFO]\t[TRAIN] epoch=1, iter=1400/36000, loss=0.5209, lr=0.009653, batch_cost=0.8582, reader_cost=0.00018, ips=37.2894 samples/sec | ETA 08:14:52\n",
      "2021-08-03 12:31:11 [INFO]\t[TRAIN] epoch=1, iter=1500/36000, loss=0.5180, lr=0.009628, batch_cost=0.8643, reader_cost=0.00017, ips=37.0242 samples/sec | ETA 08:16:58\n",
      "2021-08-03 12:32:38 [INFO]\t[TRAIN] epoch=2, iter=1600/36000, loss=0.5114, lr=0.009603, batch_cost=0.8691, reader_cost=0.00622, ips=36.8187 samples/sec | ETA 08:18:17\n",
      "2021-08-03 12:34:05 [INFO]\t[TRAIN] epoch=2, iter=1700/36000, loss=0.5013, lr=0.009578, batch_cost=0.8652, reader_cost=0.00026, ips=36.9854 samples/sec | ETA 08:14:36\n",
      "2021-08-03 12:35:31 [INFO]\t[TRAIN] epoch=2, iter=1800/36000, loss=0.4998, lr=0.009554, batch_cost=0.8605, reader_cost=0.00018, ips=37.1880 samples/sec | ETA 08:10:28\n",
      "2021-08-03 12:36:57 [INFO]\t[TRAIN] epoch=2, iter=1900/36000, loss=0.5034, lr=0.009529, batch_cost=0.8634, reader_cost=0.00018, ips=37.0644 samples/sec | ETA 08:10:40\n",
      "2021-08-03 12:38:23 [INFO]\t[TRAIN] epoch=2, iter=2000/36000, loss=0.4929, lr=0.009504, batch_cost=0.8571, reader_cost=0.00017, ips=37.3362 samples/sec | ETA 08:05:40\n",
      "2021-08-03 12:38:23 [INFO]\tStart evaluating (total_samples=20, total_iters=20)...\n",
      "20/20 [==============================] - 4s 186ms/step - batch_cost: 0.1861 - reader cost: 7.1533e-0\n",
      "2021-08-03 12:38:27 [INFO]\t[EVAL] #Images=20 mIoU=0.1229 Acc=0.1229 Kappa=0.1229 \n",
      "2021-08-03 12:38:27 [INFO]\t[EVAL] Class IoU: \n",
      "0.1229\n",
      "2021-08-03 12:38:27 [INFO]\t[EVAL] Class Acc: \n",
      "0.1229\n",
      "2021-08-03 12:38:37 [INFO]\t[EVAL] The model with the best validation mIoU (0.1229) was saved at iter 2000.\n",
      "2021-08-03 12:40:03 [INFO]\t[TRAIN] epoch=2, iter=2100/36000, loss=0.5008, lr=0.009479, batch_cost=0.8622, reader_cost=0.00018, ips=37.1143 samples/sec | ETA 08:07:08\n",
      "2021-08-03 12:41:30 [INFO]\t[TRAIN] epoch=2, iter=2200/36000, loss=0.4942, lr=0.009454, batch_cost=0.8679, reader_cost=0.00017, ips=36.8685 samples/sec | ETA 08:08:56\n",
      "2021-08-03 12:42:56 [INFO]\t[TRAIN] epoch=2, iter=2300/36000, loss=0.4980, lr=0.009429, batch_cost=0.8617, reader_cost=0.00018, ips=37.1366 samples/sec | ETA 08:03:58\n",
      "2021-08-03 12:44:22 [INFO]\t[TRAIN] epoch=2, iter=2400/36000, loss=0.4893, lr=0.009404, batch_cost=0.8608, reader_cost=0.00017, ips=37.1761 samples/sec | ETA 08:02:01\n",
      "2021-08-03 12:45:48 [INFO]\t[TRAIN] epoch=2, iter=2500/36000, loss=0.4863, lr=0.009379, batch_cost=0.8619, reader_cost=0.00017, ips=37.1289 samples/sec | ETA 08:01:12\n",
      "2021-08-03 12:47:14 [INFO]\t[TRAIN] epoch=2, iter=2600/36000, loss=0.4844, lr=0.009354, batch_cost=0.8575, reader_cost=0.00017, ips=37.3173 samples/sec | ETA 07:57:20\n",
      "2021-08-03 12:48:40 [INFO]\t[TRAIN] epoch=2, iter=2700/36000, loss=0.4902, lr=0.009329, batch_cost=0.8608, reader_cost=0.00018, ips=37.1744 samples/sec | ETA 07:57:44\n",
      "2021-08-03 12:50:06 [INFO]\t[TRAIN] epoch=2, iter=2800/36000, loss=0.4810, lr=0.009304, batch_cost=0.8585, reader_cost=0.00026, ips=37.2738 samples/sec | ETA 07:55:02\n",
      "2021-08-03 12:51:32 [INFO]\t[TRAIN] epoch=2, iter=2900/36000, loss=0.4889, lr=0.009280, batch_cost=0.8629, reader_cost=0.00018, ips=37.0862 samples/sec | ETA 07:56:00\n",
      "2021-08-03 12:52:58 [INFO]\t[TRAIN] epoch=2, iter=3000/36000, loss=0.4783, lr=0.009255, batch_cost=0.8581, reader_cost=0.00017, ips=37.2938 samples/sec | ETA 07:51:55\n",
      "2021-08-03 12:54:23 [INFO]\t[TRAIN] epoch=2, iter=3100/36000, loss=0.4825, lr=0.009230, batch_cost=0.8556, reader_cost=0.00017, ips=37.4007 samples/sec | ETA 07:49:09\n",
      "2021-08-03 12:55:50 [INFO]\t[TRAIN] epoch=3, iter=3200/36000, loss=0.4693, lr=0.009205, batch_cost=0.8690, reader_cost=0.00564, ips=36.8236 samples/sec | ETA 07:55:03\n",
      "2021-08-03 12:57:16 [INFO]\t[TRAIN] epoch=3, iter=3300/36000, loss=0.4765, lr=0.009180, batch_cost=0.8556, reader_cost=0.00016, ips=37.3995 samples/sec | ETA 07:46:18\n",
      "2021-08-03 12:58:41 [INFO]\t[TRAIN] epoch=3, iter=3400/36000, loss=0.4774, lr=0.009155, batch_cost=0.8564, reader_cost=0.00016, ips=37.3640 samples/sec | ETA 07:45:19\n",
      "2021-08-03 13:00:07 [INFO]\t[TRAIN] epoch=3, iter=3500/36000, loss=0.4745, lr=0.009130, batch_cost=0.8580, reader_cost=0.00017, ips=37.2945 samples/sec | ETA 07:44:46\n",
      "2021-08-03 13:01:32 [INFO]\t[TRAIN] epoch=3, iter=3600/36000, loss=0.4711, lr=0.009105, batch_cost=0.8516, reader_cost=0.00016, ips=37.5777 samples/sec | ETA 07:39:50\n",
      "2021-08-03 13:02:58 [INFO]\t[TRAIN] epoch=3, iter=3700/36000, loss=0.4718, lr=0.009080, batch_cost=0.8594, reader_cost=0.00017, ips=37.2337 samples/sec | ETA 07:42:39\n",
      "2021-08-03 13:04:24 [INFO]\t[TRAIN] epoch=3, iter=3800/36000, loss=0.4707, lr=0.009055, batch_cost=0.8600, reader_cost=0.00017, ips=37.2080 samples/sec | ETA 07:41:32\n",
      "2021-08-03 13:05:50 [INFO]\t[TRAIN] epoch=3, iter=3900/36000, loss=0.4714, lr=0.009030, batch_cost=0.8581, reader_cost=0.00017, ips=37.2935 samples/sec | ETA 07:39:03\n",
      "2021-08-03 13:07:16 [INFO]\t[TRAIN] epoch=3, iter=4000/36000, loss=0.4695, lr=0.009005, batch_cost=0.8589, reader_cost=0.00018, ips=37.2557 samples/sec | ETA 07:38:05\n",
      "2021-08-03 13:07:16 [INFO]\tStart evaluating (total_samples=20, total_iters=20)...\n",
      "20/20 [==============================] - 4s 191ms/step - batch_cost: 0.1905 - reader cost: 6.8058e-0\n",
      "2021-08-03 13:07:20 [INFO]\t[EVAL] #Images=20 mIoU=0.1093 Acc=0.1093 Kappa=0.1093 \n",
      "2021-08-03 13:07:20 [INFO]\t[EVAL] Class IoU: \n",
      "0.1093\n",
      "2021-08-03 13:07:20 [INFO]\t[EVAL] Class Acc: \n",
      "0.1093\n",
      "2021-08-03 13:07:30 [INFO]\t[EVAL] The model with the best validation mIoU (0.1093) was saved at iter 4000.\n",
      "2021-08-03 13:08:56 [INFO]\t[TRAIN] epoch=3, iter=4100/36000, loss=0.4717, lr=0.008979, batch_cost=0.8622, reader_cost=0.00017, ips=37.1130 samples/sec | ETA 07:38:25\n",
      "2021-08-03 13:10:23 [INFO]\t[TRAIN] epoch=3, iter=4200/36000, loss=0.4725, lr=0.008954, batch_cost=0.8649, reader_cost=0.00018, ips=36.9977 samples/sec | ETA 07:38:24\n",
      "2021-08-03 13:11:49 [INFO]\t[TRAIN] epoch=3, iter=4300/36000, loss=0.4698, lr=0.008929, batch_cost=0.8623, reader_cost=0.00025, ips=37.1103 samples/sec | ETA 07:35:34\n",
      "2021-08-03 13:13:15 [INFO]\t[TRAIN] epoch=3, iter=4400/36000, loss=0.4662, lr=0.008904, batch_cost=0.8662, reader_cost=0.00018, ips=36.9414 samples/sec | ETA 07:36:13\n",
      "2021-08-03 13:14:42 [INFO]\t[TRAIN] epoch=3, iter=4500/36000, loss=0.4656, lr=0.008879, batch_cost=0.8685, reader_cost=0.00018, ips=36.8446 samples/sec | ETA 07:35:58\n",
      "2021-08-03 13:16:08 [INFO]\t[TRAIN] epoch=3, iter=4600/36000, loss=0.4675, lr=0.008854, batch_cost=0.8588, reader_cost=0.00017, ips=37.2615 samples/sec | ETA 07:29:26\n",
      "2021-08-03 13:17:35 [INFO]\t[TRAIN] epoch=4, iter=4700/36000, loss=0.4533, lr=0.008829, batch_cost=0.8659, reader_cost=0.00610, ips=36.9579 samples/sec | ETA 07:31:41\n",
      "2021-08-03 13:19:00 [INFO]\t[TRAIN] epoch=4, iter=4800/36000, loss=0.4552, lr=0.008804, batch_cost=0.8513, reader_cost=0.00015, ips=37.5901 samples/sec | ETA 07:22:40\n",
      "2021-08-03 13:20:26 [INFO]\t[TRAIN] epoch=4, iter=4900/36000, loss=0.4578, lr=0.008779, batch_cost=0.8606, reader_cost=0.00017, ips=37.1820 samples/sec | ETA 07:26:05\n",
      "2021-08-03 13:21:52 [INFO]\t[TRAIN] epoch=4, iter=5000/36000, loss=0.4654, lr=0.008754, batch_cost=0.8583, reader_cost=0.00017, ips=37.2835 samples/sec | ETA 07:23:26\n",
      "2021-08-03 13:23:18 [INFO]\t[TRAIN] epoch=4, iter=5100/36000, loss=0.4617, lr=0.008729, batch_cost=0.8593, reader_cost=0.00018, ips=37.2378 samples/sec | ETA 07:22:33\n",
      "2021-08-03 13:24:44 [INFO]\t[TRAIN] epoch=4, iter=5200/36000, loss=0.4545, lr=0.008703, batch_cost=0.8646, reader_cost=0.00017, ips=37.0106 samples/sec | ETA 07:23:50\n",
      "2021-08-03 13:26:10 [INFO]\t[TRAIN] epoch=4, iter=5300/36000, loss=0.4617, lr=0.008678, batch_cost=0.8549, reader_cost=0.00016, ips=37.4299 samples/sec | ETA 07:17:26\n",
      "2021-08-03 13:27:36 [INFO]\t[TRAIN] epoch=4, iter=5400/36000, loss=0.4607, lr=0.008653, batch_cost=0.8639, reader_cost=0.00018, ips=37.0398 samples/sec | ETA 07:20:36\n",
      "2021-08-03 13:29:02 [INFO]\t[TRAIN] epoch=4, iter=5500/36000, loss=0.4588, lr=0.008628, batch_cost=0.8603, reader_cost=0.00019, ips=37.1982 samples/sec | ETA 07:17:17\n",
      "2021-08-03 13:30:28 [INFO]\t[TRAIN] epoch=4, iter=5600/36000, loss=0.4583, lr=0.008603, batch_cost=0.8605, reader_cost=0.00018, ips=37.1879 samples/sec | ETA 07:15:59\n",
      "2021-08-03 13:31:54 [INFO]\t[TRAIN] epoch=4, iter=5700/36000, loss=0.4582, lr=0.008578, batch_cost=0.8555, reader_cost=0.00017, ips=37.4044 samples/sec | ETA 07:12:02\n",
      "2021-08-03 13:33:20 [INFO]\t[TRAIN] epoch=4, iter=5800/36000, loss=0.4557, lr=0.008552, batch_cost=0.8661, reader_cost=0.00018, ips=36.9459 samples/sec | ETA 07:15:57\n",
      "2021-08-03 13:34:46 [INFO]\t[TRAIN] epoch=4, iter=5900/36000, loss=0.4552, lr=0.008527, batch_cost=0.8559, reader_cost=0.00017, ips=37.3865 samples/sec | ETA 07:09:23\n",
      "2021-08-03 13:36:11 [INFO]\t[TRAIN] epoch=4, iter=6000/36000, loss=0.4586, lr=0.008502, batch_cost=0.8500, reader_cost=0.00016, ips=37.6486 samples/sec | ETA 07:04:58\n",
      "2021-08-03 13:36:11 [INFO]\tStart evaluating (total_samples=20, total_iters=20)...\n",
      "20/20 [==============================] - 4s 187ms/step - batch_cost: 0.1869 - reader cost: 7.0639e-0\n",
      "2021-08-03 13:36:15 [INFO]\t[EVAL] #Images=20 mIoU=0.1030 Acc=0.1030 Kappa=0.1030 \n",
      "2021-08-03 13:36:15 [INFO]\t[EVAL] Class IoU: \n",
      "0.103\n",
      "2021-08-03 13:36:15 [INFO]\t[EVAL] Class Acc: \n",
      "0.103\n",
      "2021-08-03 13:36:25 [INFO]\t[EVAL] The model with the best validation mIoU (0.1030) was saved at iter 6000.\n",
      "2021-08-03 13:37:51 [INFO]\t[TRAIN] epoch=4, iter=6100/36000, loss=0.4611, lr=0.008477, batch_cost=0.8672, reader_cost=0.00018, ips=36.9016 samples/sec | ETA 07:12:08\n",
      "2021-08-03 13:39:18 [INFO]\t[TRAIN] epoch=4, iter=6200/36000, loss=0.4544, lr=0.008452, batch_cost=0.8618, reader_cost=0.00018, ips=37.1302 samples/sec | ETA 07:08:02\n",
      "2021-08-03 13:40:44 [INFO]\t[TRAIN] epoch=5, iter=6300/36000, loss=0.4544, lr=0.008426, batch_cost=0.8601, reader_cost=0.00563, ips=37.2040 samples/sec | ETA 07:05:45\n",
      "2021-08-03 13:42:10 [INFO]\t[TRAIN] epoch=5, iter=6400/36000, loss=0.4473, lr=0.008401, batch_cost=0.8672, reader_cost=0.00018, ips=36.9025 samples/sec | ETA 07:07:47\n",
      "2021-08-03 13:43:37 [INFO]\t[TRAIN] epoch=5, iter=6500/36000, loss=0.4624, lr=0.008376, batch_cost=0.8632, reader_cost=0.00018, ips=37.0724 samples/sec | ETA 07:04:23\n",
      "2021-08-03 13:45:02 [INFO]\t[TRAIN] epoch=5, iter=6600/36000, loss=0.4556, lr=0.008351, batch_cost=0.8573, reader_cost=0.00017, ips=37.3272 samples/sec | ETA 07:00:04\n",
      "2021-08-03 13:46:28 [INFO]\t[TRAIN] epoch=5, iter=6700/36000, loss=0.4474, lr=0.008325, batch_cost=0.8590, reader_cost=0.00018, ips=37.2529 samples/sec | ETA 06:59:28\n",
      "2021-08-03 13:47:54 [INFO]\t[TRAIN] epoch=5, iter=6800/36000, loss=0.4510, lr=0.008300, batch_cost=0.8610, reader_cost=0.00018, ips=37.1671 samples/sec | ETA 06:59:00\n",
      "2021-08-03 13:49:20 [INFO]\t[TRAIN] epoch=5, iter=6900/36000, loss=0.4493, lr=0.008275, batch_cost=0.8596, reader_cost=0.00018, ips=37.2288 samples/sec | ETA 06:56:52\n",
      "2021-08-03 13:50:47 [INFO]\t[TRAIN] epoch=5, iter=7000/36000, loss=0.4538, lr=0.008250, batch_cost=0.8625, reader_cost=0.00018, ips=37.1020 samples/sec | ETA 06:56:52\n",
      "2021-08-03 13:52:12 [INFO]\t[TRAIN] epoch=5, iter=7100/36000, loss=0.4479, lr=0.008224, batch_cost=0.8552, reader_cost=0.00017, ips=37.4194 samples/sec | ETA 06:51:54\n",
      "2021-08-03 13:53:38 [INFO]\t[TRAIN] epoch=5, iter=7200/36000, loss=0.4492, lr=0.008199, batch_cost=0.8600, reader_cost=0.00021, ips=37.2114 samples/sec | ETA 06:52:46\n",
      "2021-08-03 13:55:04 [INFO]\t[TRAIN] epoch=5, iter=7300/36000, loss=0.4558, lr=0.008174, batch_cost=0.8549, reader_cost=0.00017, ips=37.4320 samples/sec | ETA 06:48:55\n",
      "2021-08-03 13:56:30 [INFO]\t[TRAIN] epoch=5, iter=7400/36000, loss=0.4537, lr=0.008148, batch_cost=0.8621, reader_cost=0.00017, ips=37.1187 samples/sec | ETA 06:50:56\n",
      "2021-08-03 13:57:56 [INFO]\t[TRAIN] epoch=5, iter=7500/36000, loss=0.4454, lr=0.008123, batch_cost=0.8639, reader_cost=0.00018, ips=37.0397 samples/sec | ETA 06:50:22\n",
      "2021-08-03 13:59:22 [INFO]\t[TRAIN] epoch=5, iter=7600/36000, loss=0.4461, lr=0.008098, batch_cost=0.8619, reader_cost=0.00018, ips=37.1292 samples/sec | ETA 06:47:56\n",
      "2021-08-03 14:00:49 [INFO]\t[TRAIN] epoch=5, iter=7700/36000, loss=0.4437, lr=0.008072, batch_cost=0.8659, reader_cost=0.00019, ips=36.9546 samples/sec | ETA 06:48:25\n",
      "2021-08-03 14:02:15 [INFO]\t[TRAIN] epoch=5, iter=7800/36000, loss=0.4446, lr=0.008047, batch_cost=0.8581, reader_cost=0.00017, ips=37.2922 samples/sec | ETA 06:43:18\n",
      "2021-08-03 14:03:41 [INFO]\t[TRAIN] epoch=6, iter=7900/36000, loss=0.4380, lr=0.008022, batch_cost=0.8644, reader_cost=0.00559, ips=37.0190 samples/sec | ETA 06:44:50\n",
      "2021-08-03 14:05:08 [INFO]\t[TRAIN] epoch=6, iter=8000/36000, loss=0.4461, lr=0.007996, batch_cost=0.8686, reader_cost=0.00018, ips=36.8399 samples/sec | ETA 06:45:21\n",
      "2021-08-03 14:05:08 [INFO]\tStart evaluating (total_samples=20, total_iters=20)...\n",
      "20/20 [==============================] - 4s 190ms/step - batch_cost: 0.1899 - reader cost: 6.8763e-0\n",
      "2021-08-03 14:05:12 [INFO]\t[EVAL] #Images=20 mIoU=0.0886 Acc=0.0886 Kappa=0.0886 \n",
      "2021-08-03 14:05:12 [INFO]\t[EVAL] Class IoU: \n",
      "0.0886\n",
      "2021-08-03 14:05:12 [INFO]\t[EVAL] Class Acc: \n",
      "0.0886\n",
      "2021-08-03 14:05:22 [INFO]\t[EVAL] The model with the best validation mIoU (0.0886) was saved at iter 8000.\n",
      "2021-08-03 14:06:48 [INFO]\t[TRAIN] epoch=6, iter=8100/36000, loss=0.4428, lr=0.007971, batch_cost=0.8567, reader_cost=0.00017, ips=37.3515 samples/sec | ETA 06:38:22\n",
      "2021-08-03 14:08:14 [INFO]\t[TRAIN] epoch=6, iter=8200/36000, loss=0.4450, lr=0.007945, batch_cost=0.8626, reader_cost=0.00018, ips=37.0983 samples/sec | ETA 06:39:39\n",
      "2021-08-03 14:09:40 [INFO]\t[TRAIN] epoch=6, iter=8300/36000, loss=0.4411, lr=0.007920, batch_cost=0.8548, reader_cost=0.00016, ips=37.4337 samples/sec | ETA 06:34:39\n",
      "2021-08-03 14:11:06 [INFO]\t[TRAIN] epoch=6, iter=8400/36000, loss=0.4494, lr=0.007895, batch_cost=0.8597, reader_cost=0.00018, ips=37.2209 samples/sec | ETA 06:35:28\n",
      "2021-08-03 14:12:31 [INFO]\t[TRAIN] epoch=6, iter=8500/36000, loss=0.4485, lr=0.007869, batch_cost=0.8564, reader_cost=0.00017, ips=37.3647 samples/sec | ETA 06:32:31\n",
      "2021-08-03 14:13:57 [INFO]\t[TRAIN] epoch=6, iter=8600/36000, loss=0.4473, lr=0.007844, batch_cost=0.8606, reader_cost=0.00017, ips=37.1853 samples/sec | ETA 06:32:59\n",
      "2021-08-03 14:15:24 [INFO]\t[TRAIN] epoch=6, iter=8700/36000, loss=0.4406, lr=0.007818, batch_cost=0.8634, reader_cost=0.00017, ips=37.0649 samples/sec | ETA 06:32:49\n",
      "2021-08-03 14:16:49 [INFO]\t[TRAIN] epoch=6, iter=8800/36000, loss=0.4448, lr=0.007793, batch_cost=0.8548, reader_cost=0.00017, ips=37.4355 samples/sec | ETA 06:27:30\n",
      "2021-08-03 14:18:14 [INFO]\t[TRAIN] epoch=6, iter=8900/36000, loss=0.4393, lr=0.007767, batch_cost=0.8522, reader_cost=0.00016, ips=37.5481 samples/sec | ETA 06:24:55\n",
      "2021-08-03 14:19:40 [INFO]\t[TRAIN] epoch=6, iter=9000/36000, loss=0.4464, lr=0.007742, batch_cost=0.8533, reader_cost=0.00017, ips=37.4993 samples/sec | ETA 06:24:00\n",
      "2021-08-03 14:21:06 [INFO]\t[TRAIN] epoch=6, iter=9100/36000, loss=0.4381, lr=0.007716, batch_cost=0.8602, reader_cost=0.00017, ips=37.1987 samples/sec | ETA 06:25:40\n",
      "2021-08-03 14:22:32 [INFO]\t[TRAIN] epoch=6, iter=9200/36000, loss=0.4557, lr=0.007691, batch_cost=0.8593, reader_cost=0.00017, ips=37.2380 samples/sec | ETA 06:23:50\n",
      "2021-08-03 14:23:57 [INFO]\t[TRAIN] epoch=6, iter=9300/36000, loss=0.4431, lr=0.007666, batch_cost=0.8589, reader_cost=0.00017, ips=37.2559 samples/sec | ETA 06:22:13\n",
      "2021-08-03 14:25:24 [INFO]\t[TRAIN] epoch=7, iter=9400/36000, loss=0.4406, lr=0.007640, batch_cost=0.8696, reader_cost=0.00859, ips=36.7980 samples/sec | ETA 06:25:31\n",
      "2021-08-03 14:26:50 [INFO]\t[TRAIN] epoch=7, iter=9500/36000, loss=0.4377, lr=0.007614, batch_cost=0.8590, reader_cost=0.00017, ips=37.2516 samples/sec | ETA 06:19:24\n",
      "2021-08-03 14:28:17 [INFO]\t[TRAIN] epoch=7, iter=9600/36000, loss=0.4397, lr=0.007589, batch_cost=0.8618, reader_cost=0.00017, ips=37.1296 samples/sec | ETA 06:19:12\n",
      "2021-08-03 14:29:43 [INFO]\t[TRAIN] epoch=7, iter=9700/36000, loss=0.4356, lr=0.007563, batch_cost=0.8647, reader_cost=0.00018, ips=37.0076 samples/sec | ETA 06:19:01\n",
      "2021-08-03 14:31:09 [INFO]\t[TRAIN] epoch=7, iter=9800/36000, loss=0.4376, lr=0.007538, batch_cost=0.8615, reader_cost=0.00019, ips=37.1462 samples/sec | ETA 06:16:10\n",
      "2021-08-03 14:32:35 [INFO]\t[TRAIN] epoch=7, iter=9900/36000, loss=0.4405, lr=0.007512, batch_cost=0.8633, reader_cost=0.00017, ips=37.0685 samples/sec | ETA 06:15:31\n",
      "2021-08-03 14:34:02 [INFO]\t[TRAIN] epoch=7, iter=10000/36000, loss=0.4452, lr=0.007487, batch_cost=0.8637, reader_cost=0.00017, ips=37.0479 samples/sec | ETA 06:14:17\n",
      "2021-08-03 14:34:02 [INFO]\tStart evaluating (total_samples=20, total_iters=20)...\n",
      "20/20 [==============================] - 4s 184ms/step - batch_cost: 0.1834 - reader cost: 6.8208e-0\n",
      "2021-08-03 14:34:06 [INFO]\t[EVAL] #Images=20 mIoU=0.0970 Acc=0.0970 Kappa=0.0970 \n",
      "2021-08-03 14:34:06 [INFO]\t[EVAL] Class IoU: \n",
      "0.097\n",
      "2021-08-03 14:34:06 [INFO]\t[EVAL] Class Acc: \n",
      "0.097\n",
      "2021-08-03 14:34:12 [INFO]\t[EVAL] The model with the best validation mIoU (0.0886) was saved at iter 8000.\n",
      "2021-08-03 14:35:38 [INFO]\t[TRAIN] epoch=7, iter=10100/36000, loss=0.4343, lr=0.007461, batch_cost=0.8624, reader_cost=0.00018, ips=37.1079 samples/sec | ETA 06:12:14\n",
      "2021-08-03 14:37:04 [INFO]\t[TRAIN] epoch=7, iter=10200/36000, loss=0.4391, lr=0.007436, batch_cost=0.8558, reader_cost=0.00017, ips=37.3901 samples/sec | ETA 06:08:00\n",
      "2021-08-03 14:38:30 [INFO]\t[TRAIN] epoch=7, iter=10300/36000, loss=0.4358, lr=0.007410, batch_cost=0.8583, reader_cost=0.00018, ips=37.2826 samples/sec | ETA 06:07:38\n",
      "2021-08-03 14:39:55 [INFO]\t[TRAIN] epoch=7, iter=10400/36000, loss=0.4433, lr=0.007384, batch_cost=0.8556, reader_cost=0.00017, ips=37.4024 samples/sec | ETA 06:05:02\n",
      "2021-08-03 14:41:21 [INFO]\t[TRAIN] epoch=7, iter=10500/36000, loss=0.4351, lr=0.007359, batch_cost=0.8600, reader_cost=0.00017, ips=37.2114 samples/sec | ETA 06:05:28\n",
      "2021-08-03 14:42:47 [INFO]\t[TRAIN] epoch=7, iter=10600/36000, loss=0.4395, lr=0.007333, batch_cost=0.8565, reader_cost=0.00016, ips=37.3622 samples/sec | ETA 06:02:34\n",
      "2021-08-03 14:44:13 [INFO]\t[TRAIN] epoch=7, iter=10700/36000, loss=0.4374, lr=0.007308, batch_cost=0.8578, reader_cost=0.00017, ips=37.3036 samples/sec | ETA 06:01:43\n",
      "2021-08-03 14:45:39 [INFO]\t[TRAIN] epoch=7, iter=10800/36000, loss=0.4387, lr=0.007282, batch_cost=0.8621, reader_cost=0.00017, ips=37.1186 samples/sec | ETA 06:02:04\n",
      "2021-08-03 14:47:04 [INFO]\t[TRAIN] epoch=7, iter=10900/36000, loss=0.4367, lr=0.007256, batch_cost=0.8521, reader_cost=0.00017, ips=37.5547 samples/sec | ETA 05:56:27\n",
      "2021-08-03 14:48:30 [INFO]\t[TRAIN] epoch=8, iter=11000/36000, loss=0.4413, lr=0.007231, batch_cost=0.8617, reader_cost=0.00594, ips=37.1366 samples/sec | ETA 05:59:02\n",
      "2021-08-03 14:49:56 [INFO]\t[TRAIN] epoch=8, iter=11100/36000, loss=0.4339, lr=0.007205, batch_cost=0.8567, reader_cost=0.00017, ips=37.3525 samples/sec | ETA 05:55:31\n",
      "2021-08-03 14:51:21 [INFO]\t[TRAIN] epoch=8, iter=11200/36000, loss=0.4361, lr=0.007179, batch_cost=0.8538, reader_cost=0.00017, ips=37.4811 samples/sec | ETA 05:52:53\n",
      "2021-08-03 14:52:47 [INFO]\t[TRAIN] epoch=8, iter=11300/36000, loss=0.4303, lr=0.007154, batch_cost=0.8594, reader_cost=0.00017, ips=37.2365 samples/sec | ETA 05:53:46\n",
      "2021-08-03 14:54:13 [INFO]\t[TRAIN] epoch=8, iter=11400/36000, loss=0.4370, lr=0.007128, batch_cost=0.8537, reader_cost=0.00017, ips=37.4841 samples/sec | ETA 05:50:00\n",
      "2021-08-03 14:55:39 [INFO]\t[TRAIN] epoch=8, iter=11500/36000, loss=0.4349, lr=0.007102, batch_cost=0.8626, reader_cost=0.00018, ips=37.0965 samples/sec | ETA 05:52:14\n",
      "2021-08-03 14:57:05 [INFO]\t[TRAIN] epoch=8, iter=11600/36000, loss=0.4340, lr=0.007076, batch_cost=0.8602, reader_cost=0.00017, ips=37.2016 samples/sec | ETA 05:49:48\n",
      "2021-08-03 14:58:31 [INFO]\t[TRAIN] epoch=8, iter=11700/36000, loss=0.4268, lr=0.007051, batch_cost=0.8565, reader_cost=0.00016, ips=37.3615 samples/sec | ETA 05:46:52\n",
      "2021-08-03 14:59:56 [INFO]\t[TRAIN] epoch=8, iter=11800/36000, loss=0.4281, lr=0.007025, batch_cost=0.8543, reader_cost=0.00016, ips=37.4580 samples/sec | ETA 05:44:33\n",
      "2021-08-03 15:01:22 [INFO]\t[TRAIN] epoch=8, iter=11900/36000, loss=0.4344, lr=0.006999, batch_cost=0.8569, reader_cost=0.00017, ips=37.3437 samples/sec | ETA 05:44:11\n",
      "2021-08-03 15:02:48 [INFO]\t[TRAIN] epoch=8, iter=12000/36000, loss=0.4292, lr=0.006973, batch_cost=0.8573, reader_cost=0.00017, ips=37.3275 samples/sec | ETA 05:42:54\n",
      "2021-08-03 15:02:48 [INFO]\tStart evaluating (total_samples=20, total_iters=20)...\n",
      "20/20 [==============================] - 4s 184ms/step - batch_cost: 0.1833 - reader cost: 7.0792e-0\n",
      "2021-08-03 15:02:51 [INFO]\t[EVAL] #Images=20 mIoU=0.0884 Acc=0.0884 Kappa=0.0884 \n",
      "2021-08-03 15:02:51 [INFO]\t[EVAL] Class IoU: \n",
      "0.0884\n",
      "2021-08-03 15:02:51 [INFO]\t[EVAL] Class Acc: \n",
      "0.0884\n",
      "2021-08-03 15:03:02 [INFO]\t[EVAL] The model with the best validation mIoU (0.0884) was saved at iter 12000.\n",
      "2021-08-03 15:04:27 [INFO]\t[TRAIN] epoch=8, iter=12100/36000, loss=0.4430, lr=0.006948, batch_cost=0.8552, reader_cost=0.00021, ips=37.4171 samples/sec | ETA 05:40:39\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# 这是我自己添加的第一个模型及其配置文件，模型的具体细节可以在 /home/aistudio/external-libraries/paddle_seg_semi1/paddleseg/models/FMFNetwork.py 这个目录里看到\r\n",
    "# 简要介绍一下模型、损失函数、数据增强、学习率和训练策略以及出现的一些问题\r\n",
    "# 模型：我们采用FCN架构，以paddlecls中提供的res2net200以及其预训练权重做为骨干网络来提取特征信息，输入数据为3×256×256的图片，四层layer我们分别提取了256×64×64、 512×32×32 、 1024×16×16、 2048×8×8的特征信息\r\n",
    "     # 本次比赛的要求是生成具有目标的单通道图，一般的做法为四层特征信息的融合，即encoder-decoder架构，我们也是采用这样的方法，只不过在中间添加了特征优化的模块，可见 FMFNetwork.py中的语义补充模块（SSM），特征交互模块（FIM），\r\n",
    "     # 关于这俩模块我将会在另外的方案分享里具体描述，本次提交就不做过细讲解.四层特征经过融合后使用3×3卷积调整为1通道，经过sigmoid函数与上采样恢复到原尺寸输出后在loss函数（一会细讲）和Momentum优化器的优化下进行训练.\r\n",
    "# 损失函数：我们自己实现了iou损失函数在2分类中的实现 代码见：external-libraries/paddle_seg_semi1/paddleseg/models/losses/semi_loss.py\r\n",
    "# 数据增强：根据paddleseg的用法，我们并没有用过多的数据增强操作，只用了resize、 对比度亮度、水平翻转、垂直反转、随即模糊、归一化这些操作，训练数据使用随机种子固定随机数，随机选取2K个作为验证集.\r\n",
    "# 学习率：使用paddleseg推荐的poly decay 作为学习率衰减策略，初始学习率为0.01，最小学习率为0.0001，\r\n",
    "# 训练策略以及一些问题：训练策略如下，2000次迭代保存一次模型，本来打算训练2w4k次迭代，但是发现模型在1w2k次迭代效果最好，但是loss和评价指标却一直在变好，可能是存在验证集过拟合的现象，具体原因不知.\r\n",
    "\r\n",
    "# 这是训练代码，复现请配置好文件后直接运行下一个代码块\r\n",
    "%cd external-libraries/paddle_seg_semi1/\r\n",
    "!python train.py --config configs/semi/fcn_FMF_semi_50k.yml \\\r\n",
    "--do_eval \\\r\n",
    "--save_dir /home/aistudio/output/fmf \\\r\n",
    "--save_interval 2000 \\\r\n",
    "--log_iters 100  \\\r\n",
    "--use_vdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'external-libraries/paddle_seg_semi1/'\n",
      "/home/aistudio/external-libraries/paddle_seg_semi1\n",
      "/home/aistudio/external-libraries/paddle_seg_semi1/paddleseg/models/ACFF_ViT.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Callable\n",
      "2021-08-06 10:05:12 [INFO]\t\n",
      "---------------Config Information---------------\n",
      "batch_size: 32\n",
      "iters: 36000\n",
      "learning_rate:\n",
      "  decay:\n",
      "    end_lr: 0.0001\n",
      "    power: 0.9\n",
      "    type: poly\n",
      "  value: 0.01\n",
      "loss:\n",
      "  coef:\n",
      "  - 1\n",
      "  types:\n",
      "  - type: SemiLoss\n",
      "model:\n",
      "  backbone:\n",
      "    pretrained: /home/aistudio/data/data99666/Res2Net200_vd_26w_4s_ssld_pretrained.pdparams\n",
      "    type: Res2Net\n",
      "  num_classes: 2\n",
      "  type: FMFModel\n",
      "optimizer:\n",
      "  momentum: 0.9\n",
      "  type: sgd\n",
      "  weight_decay: 0.0005\n",
      "train_dataset:\n",
      "  dataset_root: /home/aistudio/data\n",
      "  mode: train\n",
      "  transforms:\n",
      "  - max_rotation: 30\n",
      "    max_scale_factor: 1.5\n",
      "    min_scale_factor: 0.5\n",
      "    size:\n",
      "    - 256\n",
      "    - 256\n",
      "    translation_offset: 128\n",
      "    type: RandomAffine\n",
      "  - type: RandomHorizontalFlip\n",
      "  - type: RandomVerticalFlip\n",
      "  - brightness_range: 0.5\n",
      "    contrast_range: 0.5\n",
      "    saturation_range: 0.5\n",
      "    type: RandomDistort\n",
      "  - type: RandomBlur\n",
      "  - mean:\n",
      "    - 0.485\n",
      "    - 0.456\n",
      "    - 0.406\n",
      "    std:\n",
      "    - 0.229\n",
      "    - 0.224\n",
      "    - 0.225\n",
      "    type: Normalize\n",
      "  type: SEMI\n",
      "val_dataset:\n",
      "  dataset_root: /home/aistudio/data\n",
      "  mode: val\n",
      "  transforms:\n",
      "  - target_size:\n",
      "    - 256\n",
      "    - 256\n",
      "    type: Resize\n",
      "  - mean:\n",
      "    - 0.485\n",
      "    - 0.456\n",
      "    - 0.406\n",
      "    std:\n",
      "    - 0.229\n",
      "    - 0.224\n",
      "    - 0.225\n",
      "    type: Normalize\n",
      "  type: SEMI\n",
      "------------------------------------------------\n",
      "W0806 10:05:12.835805   871 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0806 10:05:12.835866   871 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "# 这是预测的代码，依然是根据paddleseg套件直接运行的，采取了多尺度预测和水平翻转的预测策略， 这里就没什么好讲的了\r\n",
    "%cd external-libraries/paddle_seg_semi1/\r\n",
    "!python predict.py --config configs/semi/fcn_FMF_semi_50k.yml \\\r\n",
    "--model_path /home/aistudio/data/data99666/200_FMF.pdparams \\\r\n",
    "--image_path /home/aistudio/data/test_image/ \\\r\n",
    "--save_dir /home/aistudio/output/result_fmf \\\r\n",
    "--aug_pred --scales  1 1.5 2 \\\r\n",
    "--flip_horizontal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############# ACFF ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/external-libraries/paddle_seg_semi1\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/setuptools/depends.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "2021-07-29 21:51:18 [INFO]\t\n",
      "------------Environment Information-------------\n",
      "platform: Linux-4.4.0-166-generic-x86_64-with-debian-stretch-sid\n",
      "Python: 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0]\n",
      "Paddle compiled with cuda: False\n",
      "GCC: gcc (Ubuntu 7.5.0-3ubuntu1~16.04) 7.5.0\n",
      "PaddlePaddle: 2.1.0\n",
      "OpenCV: 4.1.1\n",
      "------------------------------------------------\n",
      "2021-07-29 21:51:35 [INFO]\t\n",
      "---------------Config Information---------------\n",
      "batch_size: 32\n",
      "iters: 24000\n",
      "learning_rate:\n",
      "  decay:\n",
      "    end_lr: 0.001\n",
      "    power: 0.9\n",
      "    type: poly\n",
      "  value: 0.01\n",
      "loss:\n",
      "  coef:\n",
      "  - 1\n",
      "  types:\n",
      "  - ignore_index: 255\n",
      "    type: SemiLoss\n",
      "model:\n",
      "  backbone:\n",
      "    layers: 200\n",
      "    pretrained: /home/aistudio/data/data99666/Res2Net200_vd_26w_4s_ssld_pretrained.pdparams\n",
      "    scales: 4\n",
      "    type: Res2Net\n",
      "    width: 26\n",
      "  num_classes: 2\n",
      "  type: ACFFModel\n",
      "optimizer:\n",
      "  momentum: 0.9\n",
      "  type: sgd\n",
      "  weight_decay: 0.0005\n",
      "train_dataset:\n",
      "  dataset_root: /home/aistudio/data\n",
      "  mode: train\n",
      "  transforms:\n",
      "  - target_size:\n",
      "    - 256\n",
      "    - 256\n",
      "    type: Resize\n",
      "  - type: RandomHorizontalFlip\n",
      "  - type: RandomVerticalFlip\n",
      "  - brightness_range: 0.5\n",
      "    contrast_range: 0.5\n",
      "    saturation_range: 0.5\n",
      "    type: RandomDistort\n",
      "  - type: RandomBlur\n",
      "  - mean:\n",
      "    - 0.485\n",
      "    - 0.456\n",
      "    - 0.406\n",
      "    std:\n",
      "    - 0.229\n",
      "    - 0.224\n",
      "    - 0.225\n",
      "    type: Normalize\n",
      "  type: SEMI\n",
      "val_dataset:\n",
      "  dataset_root: /home/aistudio/data\n",
      "  mode: val\n",
      "  transforms:\n",
      "  - target_size:\n",
      "    - 256\n",
      "    - 256\n",
      "    type: Resize\n",
      "  - mean:\n",
      "    - 0.485\n",
      "    - 0.456\n",
      "    - 0.406\n",
      "    std:\n",
      "    - 0.229\n",
      "    - 0.224\n",
      "    - 0.225\n",
      "    type: Normalize\n",
      "  type: SEMI\n",
      "------------------------------------------------\n",
      "2021-07-29 21:51:47 [INFO]\tLoading pretrained model from /home/aistudio/data/data99666/Res2Net200_vd_26w_4s_ssld_pretrained.pdparams\n",
      "2021-07-29 21:51:53 [INFO]\tThere are 1685/1685 variables loaded into Res2Net.\n",
      "init finished!\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 154, in <module>\n",
      "    main(args)\n",
      "  File \"train.py\", line 149, in main\n",
      "    keep_checkpoint_max=args.keep_checkpoint_max)\n",
      "  File \"/home/aistudio/external-libraries/paddle_seg_semi1/paddleseg/core/train.py\", line 194, in train\n",
      "    logits_list = model(images)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 898, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/aistudio/external-libraries/paddle_seg_semi1/paddleseg/models/ACFFNetwork.py\", line 411, in forward\n",
      "    x1, x2, x3, x4, x5 = self.backbone(x)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 898, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/home/aistudio/external-libraries/paddle_seg_semi1/paddleseg/models/FMFNetwork.py\", line 423, in forward\n",
      "    y = self.pool2d_max(y)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\", line 898, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/pooling.py\", line 518, in forward\n",
      "    name=self.name)\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/functional/pooling.py\", line 720, in max_pool2d\n",
      "    'data_format', data_format)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# 这是我自己添加的第二个模型及其配置文件，模型的具体细节可以在 /home/aistudio/external-libraries/paddle_seg_semi1/paddleseg/models/ACFFNetwork.py 这个目录里看到\r\n",
    "# 简要介绍一下模型、损失函数、数据增强、学习率和训练策略以及出现的一些问题\r\n",
    "# 模型：我们采用FCN架构，以paddlecls中提供的res2net200以及其预训练权重做为骨干网络来提取特征信息，输入数据为3×256×256的图片，四层layer我们分别提取了256×64×64、 512×32×32 、 1024×16×16、 2048×8×8的特征信息\r\n",
    "     # 本次比赛的要求是生成具有目标的单通道图，一般的做法为四层特征信息的融合，即encoder-decoder架构，我们也是采用这样的方法，只不过在中间添加了特征优化的模块，可见 ACFFNetwork.py中的自优化模块（SR），注意力模块（SE）等，\r\n",
    "     # 关于这些模块我将会在另外的方案分享里具体描述，本次提交就不做过细讲解.四层特征经过融合后使用3×3卷积调整为1通道，经过sigmoid函数与上采样恢复到原尺寸输出后在loss函数（一会细讲）和Momentum优化器的优化下进行训练.\r\n",
    "# 损失函数：我们自己实现了iou损失函数在2分类中的实现 代码见：external-libraries/paddle_seg_semi1/paddleseg/models/losses/semi_loss.py\r\n",
    "# 数据增强：根据paddleseg的用法，我们并没有用过多的数据增强操作，只用了resize、 对比度亮度、水平翻转、垂直反转、随即模糊、归一化这些操作，训练数据使用随机种子固定随机数，随机选取2K个作为验证集.\r\n",
    "# 学习率：使用paddleseg推荐的poly decay 作为学习率衰减策略，初始学习率为0.01，最小学习率为0.0001，\r\n",
    "# 训练策略以及一些问题：训练策略如下，3000次迭代保存一次模型，本来打算训练2w4k次迭代，但是发现模型在1w2k次迭代效果最好，但是loss和评价指标却一直在变好，可能是存在验证集过拟合的现象，具体原因不知.\r\n",
    "\r\n",
    "# 这是训练代码，复现请配置好文件后直接运行下一个代码块\r\n",
    "%cd external-libraries/paddle_seg_semi1/\r\n",
    "!python train.py --config configs/semi/fcn_ACFFNet_semi_50k.yml \\\r\n",
    "--do_eval \\\r\n",
    "--save_dir /home/aistudio/output/acff \\\r\n",
    "--save_interval 3000 \\\r\n",
    "--log_iters 100  \\\r\n",
    "--use_vdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这是预测的代码，依然是根据paddleseg套件直接运行的，采取了多尺度预测和水平翻转的预测策略， 这里就没什么好讲的了\r\n",
    "%cd external-libraries/paddle_seg_semi1/\r\n",
    "!python predict.py --config configs/semi/fcn_ACFFNet_semi_50k.yml \\\r\n",
    "--model_path /home/aistudio/data/data99666/res2net200_score_0.80918.pdparams \\\r\n",
    "--image_path /home/aistudio/data/test_image/ \\\r\n",
    "--save_dir /home/aistudio/output/result_acff \\\r\n",
    "--aug_pred --scales  1 1.5 2 \\\r\n",
    "--flip_horizontal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########## ACFF ViT ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'external-libraries/paddle_seg_semi1/'\n",
      "/home/aistudio/external-libraries/paddle_seg_semi1\n",
      "/home/aistudio/external-libraries/paddle_seg_semi1/paddleseg/models/ACFF_ViT.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Callable\n",
      "2021-08-02 07:47:43 [INFO]\t\n",
      "------------Environment Information-------------\n",
      "platform: Linux-4.4.0-150-generic-x86_64-with-debian-stretch-sid\n",
      "Python: 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0]\n",
      "Paddle compiled with cuda: True\n",
      "NVCC: Cuda compilation tools, release 10.1, V10.1.243\n",
      "cudnn: 7.6\n",
      "GPUs used: 1\n",
      "CUDA_VISIBLE_DEVICES: None\n",
      "GPU: ['GPU 0: Tesla V100-SXM2-32GB']\n",
      "GCC: gcc (Ubuntu 7.5.0-3ubuntu1~16.04) 7.5.0\n",
      "PaddlePaddle: 2.1.0\n",
      "OpenCV: 4.1.1\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 这是我自己添加的第三个模型及其配置文件，模型的具体细节可以在 /home/aistudio/external-libraries/paddle_seg_semi1/paddleseg/models/ACFF_ViT.py 这个目录里看到\r\n",
    "# 简要介绍一下模型、损失函数、数据增强、学习率和训练策略以及出现的一些问题\r\n",
    "# 模型：我们采用FCN架构，以paddlecls中提供的SwinTrasnformer以及其预训练权重做为骨干网络来提取特征信息，输入数据为3×384×384的图片，四层layer我们分别提取了不同大小不同通道数的四层特征信息\r\n",
    "     # 本次比赛的要求是生成具有目标的单通道图，一般的做法为四层特征信息的融合，即encoder-decoder架构，我们也是采用这样的方法，只不过在中间添加了特征优化的模块，可见 ACFF_ViT.py中的自优化模块（SR），注意力模块（SE）等，\r\n",
    "     # 关于这些模块我将会在另外的方案分享里具体描述，本次提交就不做过细讲解.四层特征经过融合后使用3×3卷积调整为1通道，经过sigmoid函数与上采样恢复到原尺寸输出后在loss函数（一会细讲）和Momentum优化器的优化下进行训练.\r\n",
    "# 损失函数：我们自己实现了iou损失函数在2分类中的实现 代码见：external-libraries/paddle_seg_semi1/paddleseg/models/losses/semi_loss.py\r\n",
    "# 数据增强：根据paddleseg的用法，我们并没有用过多的数据增强操作，只用了resize、 对比度亮度、水平翻转、垂直反转、随即模糊、归一化这些操作，训练数据使用随机种子固定随机数，随机选取2K个作为验证集.\r\n",
    "# 学习率：使用paddleseg推荐的poly decay 作为学习率衰减策略，初始学习率为0.01，最小学习率为0.0001，\r\n",
    "# 训练策略以及一些问题：训练策略如下，bs = 4, 4000次迭代保存一次模型，训练2个epoch后效果最好\r\n",
    "\r\n",
    "# 这是训练代码，复现请配置好文件后直接运行下一个代码块\r\n",
    "%cd external-libraries/paddle_seg_semi1/\r\n",
    "!python train.py --config configs/semi/fcn_ACFFViT.yml \\\r\n",
    "--do_eval \\\r\n",
    "--save_dir /home/aistudio/output/vit \\\r\n",
    "--save_interval 4000 \\\r\n",
    "--log_iters 100  \\\r\n",
    "--use_vdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这是预测的代码，依然是根据paddleseg套件直接运行的，采取了水平翻转的预测策略， 因为transformer固定大小的输入模式，只能固定384尺寸输入 所以没有采用多尺度预测\r\n",
    "%cd external-libraries/paddle_seg_semi1/\r\n",
    "!python predict.py --config configs/semi/fcn_ACFFViT.yml \\\r\n",
    "--model_path /home/aistudio/data/data99666/ACFFViT4.pdparams\\\r\n",
    "--image_path /home/aistudio/data/test_image \\\r\n",
    "--save_dir /home/aistudio/output/result_vit \\\r\n",
    "--aug_pred \\\r\n",
    "--flip_horizontal\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############## Next_ACFF ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这是我自己添加的第四个模型及其配置文件，模型的具体细节可以在 /home/aistudio/external-libraries/paddle_seg_semi1/paddleseg/models/Next_ACFF.py 这个目录里看到\r\n",
    "# 简要介绍一下模型、损失函数、数据增强、学习率和训练策略以及出现的一些问题\r\n",
    "# 模型：我们采用FCN架构，以paddlecls中提供的ResNeXt101WSL以及其预训练权重做为骨干网络来提取特征信息，输入数据为3×256×256的图片，四层layer我们分别提取了256×64×64、 512×32×32 、 1024×16×16、 2048×8×8的特征信息\r\n",
    "     # 本次比赛的要求是生成具有目标的单通道图，一般的做法为四层特征信息的融合，即encoder-decoder架构，我们也是采用这样的方法，只不过在中间添加了特征优化的模块，可见 Next_ACFF.py中的自优化模块（SR），注意力模块（SE）等，\r\n",
    "     # 关于这些模块我将会在另外的方案分享里具体描述，本次提交就不做过细讲解.四层特征经过融合后使用3×3卷积调整为1通道，经过sigmoid函数与上采样恢复到原尺寸输出后在loss函数（一会细讲）和Momentum优化器的优化下进行训练.\r\n",
    "# 损失函数：我们自己实现了iou损失函数在2分类中的实现 代码见：external-libraries/paddle_seg_semi1/paddleseg/models/losses/semi_loss.py\r\n",
    "# 数据增强：根据paddleseg的用法，我们并没有用过多的数据增强操作，只用了resize、 对比度亮度、水平翻转、垂直反转、随即模糊、归一化这些操作，训练数据使用随机种子固定随机数，随机选取2K个作为验证集.\r\n",
    "# 学习率：使用paddleseg推荐的poly decay 作为学习率衰减策略，初始学习率为0.01，最小学习率为0.0001，\r\n",
    "# 训练策略以及一些问题：训练策略如下，2000次迭代保存一次模型，多少次迭代效果最好已经忘了，肯定不是最后一次，还是中间某次\r\n",
    "\r\n",
    "# 这是训练代码，复现请配置好文件后直接运行下一个代码块\r\n",
    "%cd external-libraries/paddle_seg_semi1/\r\n",
    "!python train.py --config configs/semi/fcn_NextACFF.yml \\\r\n",
    "--do_eval \\\r\n",
    "--save_dir /home/aistudio/output/next \\\r\n",
    "--save_interval 2000 \\\r\n",
    "--log_iters 100  \\\r\n",
    "--use_vdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这是预测的代码，依然是根据paddleseg套件直接运行的，采取了多尺度预测和水平翻转的预测策略， 这里就没什么好讲的了\r\n",
    "%cd external-libraries/paddle_seg_semi1/\r\n",
    "!python predict.py --config configs/semi/fcn_NextACFF.yml \\\r\n",
    "--model_path /home/aistudio/data/data99666/NextACFF.pdparams \\\r\n",
    "--image_path /home/aistudio/data/test_image/ \\\r\n",
    "--save_dir /home/aistudio/output/result_acffnext \\\r\n",
    "--aug_pred --scales  1 1.5 2 \\\r\n",
    "--flip_horizontal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 另外说一下，我们根据比赛的评价指标，修改了 external-libraries/paddle_seg_semi1/paddleseg/core/train.py、 infer.py、 predict.py、 val.py等文件\r\n",
    "# 但不影响运行，主要修改为：修改了输入输出的数据类型 int -> float32   修改了输出 多保存了图像的npy文件用于后续的模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aistudio/external-libraries/paddle_seg_semi1/paddleseg/models/ACFF_ViT.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Callable\n",
      "10985/10989 [============================>.] - ETA: 0"
     ]
    }
   ],
   "source": [
    "# 这个融合策略是上述四个模型结果取平均融合的方法，B榜结果8.0383 不是最好的结果  可以忽略， 只是我不想删，因为是我队友辛苦写出来的\r\n",
    "\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import cv2\r\n",
    "import glob\r\n",
    "\r\n",
    "from paddleseg.utils import logger, progbar\r\n",
    "\r\n",
    "save_image_path = '/home/aistudio/output/result_ensem/pre'\r\n",
    "if not os.path.exists(save_image_path):\r\n",
    "    os.makedirs(save_image_path)\r\n",
    "\r\n",
    "logits_path = [\r\n",
    "    '/home/aistudio/output/result_acff/logits',\r\n",
    "    '/home/aistudio/output/result_fmf/logits',\r\n",
    "    '/home/aistudio/output/result_vit/logits',\r\n",
    "    '/home/aistudio/output/result_acffnext/logits'\r\n",
    "]\r\n",
    "npy_list = glob.glob(os.path.join(logits_path[0], '*.npy'))\r\n",
    "npy_list = [npy.split('/')[-1] for npy in npy_list]\r\n",
    "\r\n",
    "progbar_pred = progbar.Progbar(target=len(npy_list), verbose=1)\r\n",
    "\r\n",
    "weights = [0.2, 0.2, 0.2, 0.4]\r\n",
    "\r\n",
    "for j, npy in enumerate(npy_list):\r\n",
    "    final_logit = 0\r\n",
    "    for i, path in enumerate(logits_path):\r\n",
    "        if 'vit' in path:\r\n",
    "            logit = np.load(os.path.join(path, npy)) / 2.0\r\n",
    "        else:\r\n",
    "            logit = np.load(os.path.join(path, npy)) / 6.0\r\n",
    "        final_logit += logit\r\n",
    "\r\n",
    "    final_logit = final_logit / len(logits_path)\r\n",
    "    pre = final_logit#[:, 1, :, :]\r\n",
    "    pre = pre[0,:,:]\r\n",
    "    pre = np.transpose(pre, [1, 2, 0]).squeeze(axis=-1) * 255\r\n",
    "    \r\n",
    "    cv2.imwrite(os.path.join(save_image_path, npy.split('.')[0] + '.jpg'), pre)\r\n",
    "    progbar_pred.update(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10989"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\r\n",
    "len(os.listdir('/home/aistudio/output/result_ensem/pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这个融合策略是上述四个模型结果加权融合的方法，B榜最好结果8.0428 达到最好的结果 \r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import cv2\r\n",
    "import glob\r\n",
    "\r\n",
    "from paddleseg.utils import logger, progbar\r\n",
    "\r\n",
    "save_image_path = '/home/aistudio/output/result_ensem_weight/pre'\r\n",
    "if not os.path.exists(save_image_path):\r\n",
    "    os.makedirs(save_image_path)\r\n",
    "\r\n",
    "logits_path = [\r\n",
    "    '/home/aistudio/output/result_acff/logits',\r\n",
    "    '/home/aistudio/output/result_fmf/logits',\r\n",
    "    '/home/aistudio/output/result_vit/logits',\r\n",
    "    '/home/aistudio/output/result_acffnext/logits'\r\n",
    "] # 这是上述四个模型产生的图像npy文件\r\n",
    "\r\n",
    "\r\n",
    "npy_list = glob.glob(os.path.join(logits_path[0], '*.npy'))\r\n",
    "npy_list = [npy.split('/')[-1] for npy in npy_list]\r\n",
    "\r\n",
    "progbar_pred = progbar.Progbar(target=len(npy_list), verbose=1) # paddleseg自带的进度条\r\n",
    "\r\n",
    "weights = [0.4, 0.2, 0.2, 0.2] # 四个npy文件的权重\r\n",
    "\r\n",
    "for j, npy in enumerate(npy_list):\r\n",
    "    final_logit = 0\r\n",
    "    for i, path in enumerate(logits_path):\r\n",
    "        if 'vit' in path:\r\n",
    "            logit = np.load(os.path.join(path, npy)) / 2.0  # 这是transformer为骨干网络预测的结果 /2.0是因为加了水平翻转预测\r\n",
    "        else:\r\n",
    "            logit = np.load(os.path.join(path, npy)) / 6.0 # 这是其他三个模型预测的结果 /6.0是因为加了水平翻转预测 以及三个尺度预测 即2*3 = 6\r\n",
    "        final_logit = final_logit +  logit * weights[i] # 加权融合\r\n",
    "\r\n",
    "  #  final_logit = final_logit / len(logits_path)\r\n",
    "    pre = final_logit#[:, 1, :, :]\r\n",
    "    pre = pre[0,:,:]\r\n",
    "    pre = np.transpose(pre, [1, 2, 0]).squeeze(axis=-1) * 255\r\n",
    "    \r\n",
    "    cv2.imwrite(os.path.join(save_image_path, npy.split('.')[0] + '.jpg'), pre) # 输出\r\n",
    "    progbar_pred.update(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 第一次  [0.4, 0.2, 0.2, 0.2] finished 结果记录 可忽略\r\n",
    "# 第二次 [0.2, 0.4, 0.2, 0.2] finished\r\n",
    "# 第三次 [0.2, 0.2, 0.4, 0.2] finished\r\n",
    "# 第四次 [0.2, 0.2, 0.2, 0.4] finished\r\n",
    "# 第五次 [0.5, 0.2, 0.15, 0.15] finished\r\n",
    "# 第六次 [0.6, 0.2, 0.1, 0.1] finished\r\n",
    "# 第七次 [0.5, 0.3, 0.1, 0.1] finished\r\n",
    "# 第八次 weights = [0.5, 0.1, 0.2, 0.2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
